{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Description\n",
    "The sensors were sampledat the highest rate each device would support, and we segment thisdata into non-overlapping windows of 128 time steps. We includethe data collected from the 31 smartphones in our experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-ea82c6e7c2a6>:11: DeprecationWarning: `Tracer` is deprecated since version 5.1, directly use `IPython.core.debugger.Pdb.set_trace()`\n",
      "  debug_here = Tracer()\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from IPython.core.debugger import Tracer\n",
    "scaler = StandardScaler()\n",
    "debug_here = Tracer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleaning \n",
    "data_dir = r\"D:\\My TS Datasets\\HAR_Datasets\\WISDM_ar\\WISDM_ar_v1.1\"\n",
    "data_file = \"WISDM_ar_v1.wi.txt\"\n",
    "def data_cleaning(data_dir,data_file):\n",
    "    file = open(f'{data_dir}/{data_file}', \"r\")\n",
    "    data = file.readlines()\n",
    "    data_subjects = [d.replace(';\\n', '').split(',') for d in data]\n",
    "\n",
    "    # some lines has '' at the end\n",
    "    for i in range(len(data_subjects)):\n",
    "        if data_subjects[i][-1] == \"\":\n",
    "            data_subjects[i] = data_subjects[i][:-1]\n",
    "    # some lines has ['\\n'] \n",
    "    for i in range(len(data_subjects)):\n",
    "        if data_subjects[i] == ['\\n']:\n",
    "            data_subjects.pop(i)\n",
    "    # detect rows with more than 6\n",
    "    for i,j in enumerate(data_subjects):\n",
    "        if len(j) != 6:\n",
    "            print(i)\n",
    "    df = pd.DataFrame(data_subjects, columns=('user', 'activity', 'time', 'x', 'y', 'z'))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(time_series, width, step, order='F'):\n",
    "    w = np.hstack([time_series[i:1 + i - width or None:step] for i in range(0, width)])\n",
    "    result = w.reshape((int(len(w) / width), width), order='F')\n",
    "    if order == 'F':\n",
    "        return result\n",
    "    else:\n",
    "        return np.ascontiguousarray(result)\n",
    "\n",
    "def calc_normalization(data):\n",
    "    data[np.isnan(data)] = 0\n",
    "    num_instances, num_time_steps, num_features = data.shape\n",
    "    data = np.reshape(data, (num_instances, -1))\n",
    "    scaler.fit(data)\n",
    "    return scaler\n",
    "def apply_normalization(data, scaler):\n",
    "#     scaler = StandardScaler()\n",
    "    num_instances, num_time_steps, num_features = data.shape\n",
    "    data = np.reshape(data, (num_instances, -1))\n",
    "    norm_data = scaler.transform(data)\n",
    "    norm_data[np.isnan(norm_data)] = 0\n",
    "    norm_data = np.reshape(norm_data, (num_instances, num_time_steps, num_features))\n",
    "    return norm_data\n",
    "\n",
    "def Wisdom_data_generator(data_dir):\n",
    "    full_data={}\n",
    "    seq_length = 128\n",
    "    shifting_step = 128\n",
    "    num_variables = 3\n",
    "    # dataloading \n",
    "    # Reading phones accelemeter datasets \n",
    "    data_dir = 'WISDM_ar_v1.1'\n",
    "    wisdom_refined = pd.read_csv(f'{data_dir}/Wisdom_ar.csv')\n",
    "#     df['activity'] = df['activity'].astype('category')\n",
    "#     df['labels'] = df['activity'].cat.codes\n",
    "\n",
    "    # drop irrelevant columns \n",
    "#     wisdom_refined = df.drop(columns=['time', 'activity'])\n",
    "    # drop irrelevant columns \n",
    "\n",
    "    # looping trough users \n",
    "    for user_name, user_data in  wisdom_refined.groupby('user'):\n",
    "        data, labels = [], []\n",
    "        # looping through data from each class per user\n",
    "        for class_name, class_data in user_data.groupby('labels'):\n",
    "            col_index=0\n",
    "            # slicing data to 128 with no overlap \n",
    "            sliced_data = np.empty((int(class_data.shape[0]/seq_length),seq_length, num_variables))\n",
    "            for column in class_data[['x', 'y', 'z']]:\n",
    "                column_data = class_data[column]\n",
    "                sliced_data[:,:,col_index] = sliding_window(column_data.values, seq_length,shifting_step, 'T')\n",
    "                col_index+=1\n",
    "           # append data \n",
    "            data.append(sliced_data)\n",
    "            # generate labels\n",
    "            class_labels = np.empty(sliced_data.shape[0])\n",
    "            class_labels.fill(class_name)\n",
    "            labels.append(class_labels.astype(int))\n",
    "            \n",
    "        # data and labels for each users \n",
    "        array_user_data= np.concatenate(data, axis=0 )\n",
    "        array_user_labels= np.concatenate(labels, axis=0 )        \n",
    "        \n",
    "        # Stratified train, validation, test split of the data \n",
    "        X_train, X_test, y_train, y_test = train_test_split(array_user_data, array_user_labels,  stratify=array_user_labels,  test_size=0.3,random_state=1)\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, stratify=y_train, random_state=1)\n",
    "        \n",
    "        # Data normalization \n",
    "        # Calculate mean and standard deviation based on train\n",
    "        scaler = calc_normalization (X_train)\n",
    "        \n",
    "        # Apply normalization \n",
    "        X_train = apply_normalization(X_train,scaler)\n",
    "        X_val = apply_normalization(X_val,scaler)\n",
    "        X_test = apply_normalization(X_test,scaler)\n",
    "        \n",
    "        # prepare samples\n",
    "        train_data = {'samples':X_train, 'labels':y_train}\n",
    "#         val_data   = {'samples':X_val, 'labels':y_val}\n",
    "        test_data  = {'samples':X_test, 'labels':y_test}\n",
    "        os.makedirs('Wisdom_user_data',exist_ok= True)\n",
    "        torch.save(train_data, f'wisdom_train_test/train_{user_name-1}.pt')\n",
    "#         torch.save(val_data,  f'Wisdom_user_data/val_{user_name}.pt')\n",
    "        torch.save(test_data, f'wisdom_train_test/test_{user_name-1}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_data = r\"D:\\My TS Datasets\\HAR_Datasets\\WISDM_ar\\WISDM_ar_v1.1\"\n",
    "Wisdom_data_generator(dir_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
